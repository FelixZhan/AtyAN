{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dependencies are installed when running in hosted notebooks",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099eac1d",
   "metadata": {},
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/FelixZhan/AtyAN/main/\"\n",
    "HELPER_FILES = [\n",
    "    \"analysis_utils.py\",\n",
    "    \"requirements.txt\",\n",
    "    \"BP1234-ONSET.csv\",\n",
    "]\n",
    "\n",
    "for filename in HELPER_FILES:\n",
    "    dest = Path(filename)\n",
    "    if dest.exists():\n",
    "        print(f'{filename} already present, skipping download.')\n",
    "        continue\n",
    "    print(f'Downloading {filename}...')\n",
    "    urllib.request.urlretrieve(f\"{BASE_URL}{filename}\", dest)\n",
    "print('Helper files are ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2164a275",
   "metadata": {},
   "source": [
    "## Imports and shared setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677aae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import (\n",
    "    load_base_dataset,\n",
    "    engineer_baseline_features,\n",
    "    prepare_univariate_prediction_dataset,\n",
    "    prepare_persistence_dataset,\n",
    "    run_univariate_logistic_regressions,\n",
    "    evaluate_model_zoo,\n",
    ")\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf46927",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PERSISTENCE = False  # Set True to re-enable persistence/remission analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a341cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = load_base_dataset()\n",
    "feature_df, feature_sets = engineer_baseline_features(raw_df)\n",
    "print(f'Dataset shape: {raw_df.shape}')\n",
    "print(f'Feature matrix shape: {feature_df[feature_sets[\"all_features\"]].shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebe0af",
   "metadata": {},
   "source": [
    "## Univariate prediction of future atypical AN onset\n",
    "\n",
    "Participants with full AN diagnoses or baseline atypical AN onset are removed to mirror the original risk-prediction experiment. The target labels any mBMI-defined atypical AN onset across waves 1\u20136."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be65780",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = prepare_univariate_prediction_dataset(\n",
    "    feature_df, feature_sets['all_features']\n",
    ")\n",
    "outcome_counts = prediction_df['aan_onset_anywave'].value_counts().to_dict()\n",
    "print('Univariate prediction cohort size:', len(prediction_df))\n",
    "print('Outcome counts:', outcome_counts)\n",
    "onset_logistic = run_univariate_logistic_regressions(\n",
    "    prediction_df, feature_sets['all_features'], target_col='aan_onset_anywave'\n",
    ")\n",
    "display(onset_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613392d",
   "metadata": {},
   "source": [
    "## Univariate persistence vs. remission analyses\n",
    "\n",
    "Set `RUN_PERSISTENCE = True` above to execute the persistence/remission cohort cells. The dataset retains participants with baseline or mBMI-defined onset who have complete wave-1\u20136 onset data and labels cases that revisit onset after at least one remission wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_df = None\n",
    "if RUN_PERSISTENCE:\n",
    "    persistence_df = prepare_persistence_dataset(\n",
    "        feature_df, feature_sets['all_features']\n",
    "    )\n",
    "    print('Persistence cohort size:', len(persistence_df))\n",
    "    print(persistence_df['aan_persistence'].value_counts().rename('count'))\n",
    "else:\n",
    "    print('Persistence/remission analyses are disabled. Set RUN_PERSISTENCE = True to enable them.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ab462",
   "metadata": {},
   "source": [
    "### Onset logistic summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d9684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(onset_logistic.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4086de1",
   "metadata": {},
   "source": [
    "### Onset model zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_model_metrics, onset_split_tables, _, onset_errors = evaluate_model_zoo(\n",
    "    prediction_df,\n",
    "    feature_sets['all_features'],\n",
    "    target_col='aan_onset_anywave',\n",
    "    model_names=None,\n",
    ")\n",
    "if onset_model_metrics.empty:\n",
    "    print('No onset models could be evaluated.')\n",
    "else:\n",
    "    metric_order = [\n",
    "        'test_roc_auc_mean',\n",
    "        'test_accuracy_mean',\n",
    "        'test_sensitivity_mean',\n",
    "        'test_specificity_mean',\n",
    "        'test_f_score_mean',\n",
    "        'test_g_mean_mean',\n",
    "    ]\n",
    "    present = ['model'] + [c for c in metric_order if c in onset_model_metrics.columns]\n",
    "    display(onset_model_metrics[present])\n",
    "if not onset_model_metrics.empty and 'overfit_flag' in onset_model_metrics.columns:\n",
    "    flagged = onset_model_metrics[onset_model_metrics['overfit_flag']]\n",
    "    if not flagged.empty:\n",
    "        for name in flagged['model']:\n",
    "            display(onset_split_tables[name])\n",
    "for name, err in onset_errors.items():\n",
    "    print(f'{name}: {err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efca75",
   "metadata": {},
   "source": [
    "### Persistence logistic summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4878115",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PERSISTENCE and persistence_df is not None:\n",
    "    persistence_logistic = run_univariate_logistic_regressions(\n",
    "        persistence_df,\n",
    "        feature_sets['all_features'],\n",
    "        target_col='aan_persistence',\n",
    "    )\n",
    "    display(persistence_logistic.head(20))\n",
    "else:\n",
    "    print('Skipping persistence logistic summaries.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b5737",
   "metadata": {},
   "source": [
    "### Persistence model zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c346fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_PERSISTENCE and persistence_df is not None:\n",
    "    persistence_model_metrics, persistence_split_tables, persistence_feature_tables, persistence_errors = evaluate_model_zoo(\n",
    "        persistence_df,\n",
    "        feature_sets['all_features'],\n",
    "        target_col='aan_persistence',\n",
    "        model_names=None,\n",
    "    )\n",
    "    metric_order = [\n",
    "        'test_roc_auc_mean',\n",
    "        'test_accuracy_mean',\n",
    "        'test_sensitivity_mean',\n",
    "        'test_specificity_mean',\n",
    "        'test_f_score_mean',\n",
    "        'test_g_mean_mean',\n",
    "    ]\n",
    "    if not persistence_model_metrics.empty:\n",
    "        present = ['model'] + [c for c in metric_order if c in persistence_model_metrics.columns]\n",
    "        display(persistence_model_metrics[present])\n",
    "    else:\n",
    "        print('No persistence models could be evaluated.')\n",
    "    if not persistence_model_metrics.empty and 'overfit_flag' in persistence_model_metrics.columns:\n",
    "        flagged = persistence_model_metrics[persistence_model_metrics['overfit_flag']]\n",
    "        if not flagged.empty:\n",
    "            for name in flagged['model']:\n",
    "                display(persistence_split_tables[name])\n",
    "    for label, table in persistence_feature_tables.items():\n",
    "        display(table.head(20))\n",
    "    for name, err in persistence_errors.items():\n",
    "        print(f'{name}: {err}')\n",
    "else:\n",
    "    print('Skipping persistence model zoo.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}