{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate prediction and persistence models\n",
    "\n",
    "This notebook consolidates the univariate onset-prediction workflows with the cleaned persistence/remission cohort. Both sections reuse the shared helpers so the preprocessing, cohort definitions, and logistic regression pipelines stay synchronized with the multivariate analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/FelixZhan/AtyAN/main/\"\n",
    "HELPER_FILES = [\n",
    "    \"analysis_utils.py\",\n",
    "    \"requirements.txt\",\n",
    "    \"BP1234-ONSET.csv\",\n",
    "]\n",
    "\n",
    "for filename in HELPER_FILES:\n",
    "    dest = Path(filename)\n",
    "    if dest.exists():\n",
    "        print(f'{filename} already present, skipping download.')\n",
    "        continue\n",
    "    print(f'Downloading {filename}...')\n",
    "    urllib.request.urlretrieve(f\"{BASE_URL}{filename}\", dest)\n",
    "print('Helper files are ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and shared setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from analysis_utils import (\n",
    "    load_base_dataset,\n",
    "    engineer_baseline_features,\n",
    "    prepare_univariate_prediction_dataset,\n",
    "    prepare_persistence_dataset,\n",
    "    run_univariate_logistic_regressions,\n",
    "    evaluate_model_zoo,\n",
    ")\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_subset = []  # e.g., ['tabpfn_random_forest']\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_df = load_base_dataset()\n",
    "feature_df, feature_sets = engineer_baseline_features(raw_df)\n",
    "print(f'Dataset shape: {raw_df.shape}')\n",
    "print(f'Feature matrix shape: {feature_df[feature_sets[\"all_features\"]].shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate prediction of future atypical AN onset\n",
    "\n",
    "Participants with full AN diagnoses or baseline atypical AN onset are removed to mirror the original risk-prediction experiment. The target labels any mBMI-defined atypical AN onset across waves 1\u20136."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "prediction_df = prepare_univariate_prediction_dataset(\n",
    "    feature_df, feature_sets['all_features']\n",
    ")\n",
    "outcome_counts = prediction_df['aan_onset_anywave'].value_counts().to_dict()\n",
    "print('Univariate prediction cohort size:', len(prediction_df))\n",
    "print('Outcome counts:', outcome_counts)\n",
    "prediction_results = run_univariate_logistic_regressions(\n",
    "    prediction_df, feature_sets['all_features'], target_col='aan_onset_anywave'\n",
    ")\n",
    "display(prediction_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate persistence vs. remission analyses\n",
    "\n",
    "The persistence dataset retains participants with baseline or mBMI-defined onset who have complete wave-1\u20136 onset data and labels cases that revisit onset after at least one remission wave."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "persistence_df = prepare_persistence_dataset(\n",
    "    feature_df, feature_sets['all_features']\n",
    ")\n",
    "print('Persistence cohort size:', len(persistence_df))\n",
    "print(persistence_df['aan_persistence'].value_counts().rename('count'))\n",
    "persistence_results = run_univariate_logistic_regressions(\n",
    "    persistence_df, feature_sets['all_features']\n",
    ")\n",
    "display(persistence_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onset logistic summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_logistic = run_univariate_logistic_regressions(\n",
    "    prediction_df,\n",
    "    feature_sets['all_features'],\n",
    "    target_col='aan_onset_anywave',\n",
    ")\n",
    "display(onset_logistic.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onset model zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_model_metrics, onset_split_tables, _, onset_errors = evaluate_model_zoo(\n",
    "    prediction_df,\n",
    "    feature_sets['all_features'],\n",
    "    target_col='aan_onset_anywave',\n",
    "    model_names=model_subset or None,\n",
    ")\n",
    "display(onset_model_metrics)\n",
    "if not onset_model_metrics.empty and 'overfit_flag' in onset_model_metrics.columns:\n",
    "    flagged = onset_model_metrics[onset_model_metrics['overfit_flag']]\n",
    "    if not flagged.empty:\n",
    "        for name in flagged['model']:\n",
    "            display(onset_split_tables[name])\n",
    "for name, err in onset_errors.items():\n",
    "    print(f'{name}: {err}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence logistic summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_logistic = run_univariate_logistic_regressions(\n",
    "    persistence_df,\n",
    "    feature_sets['all_features'],\n",
    "    target_col='aan_persistence',\n",
    ")\n",
    "display(persistence_logistic.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence model zoo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence_model_metrics, persistence_split_tables, persistence_feature_tables, persistence_errors = evaluate_model_zoo(\n",
    "    persistence_df,\n",
    "    feature_sets['all_features'],\n",
    "    target_col='aan_persistence',\n",
    "    model_names=model_subset or None,\n",
    ")\n",
    "display(persistence_model_metrics)\n",
    "if not persistence_model_metrics.empty and 'overfit_flag' in persistence_model_metrics.columns:\n",
    "    flagged = persistence_model_metrics[persistence_model_metrics['overfit_flag']]\n",
    "    if not flagged.empty:\n",
    "        for name in flagged['model']:\n",
    "            display(persistence_split_tables[name])\n",
    "for label, table in persistence_feature_tables.items():\n",
    "    display(table.head(20))\n",
    "for name, err in persistence_errors.items():\n",
    "    print(f'{name}: {err}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}