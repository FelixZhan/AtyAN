{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846231a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading analysis_utils.py...\n",
      "Downloading requirements.txt...\n",
      "Downloading BP1234-ONSET.csv...\n",
      "Helper files are ready.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/FelixZhan/AtyAN/main/\"\n",
    "HELPER_FILES = [\n",
    "    \"analysis_utils.py\",\n",
    "    \"requirements.txt\",\n",
    "    \"BP1234-ONSET.csv\",\n",
    "]\n",
    "\n",
    "for filename in HELPER_FILES:\n",
    "    dest = Path(filename)\n",
    "    if dest.exists():\n",
    "        print(f\"{filename} already present, skipping download.\")\n",
    "        continue\n",
    "    print(f\"Downloading {filename}...\")\n",
    "    urllib.request.urlretrieve(f\"{BASE_URL}{filename}\", dest)\n",
    "\n",
    "print(\"Helper files are ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f97cca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m551.9/551.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.3/487.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.1/222.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for tabpfn-extensions (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires notebook<7.0,>=5.7.6, but you have notebook 7.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "!pip install -q imbalanced-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00f824b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from analysis_utils import (\n",
    "    load_base_dataset,\n",
    "    engineer_baseline_features,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    ")\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ea760b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset shape: (1952, 3713)\n",
      "Feature matrix shape: (1952, 24)\n"
     ]
    }
   ],
   "source": [
    "raw_df = load_base_dataset()\n",
    "feature_df, feature_sets = engineer_baseline_features(raw_df)\n",
    "\n",
    "print(f\"Raw dataset shape: {raw_df.shape}\")\n",
    "print(f\"Feature matrix shape: {feature_df[feature_sets['all_features']].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea0769ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AAN presence columns: {1: 'w1ONSET-FULL', 2: 'w2ONSET-FULL-mBMI', 3: 'w3ONSET-FULL-mBMI', 4: 'w4ONSET-FULL-mBMI', 5: 'w5ONSET-FULL-mBMI', 6: 'w6ONSET-FULL-mBMI'}\n",
      "ID column: id\n"
     ]
    }
   ],
   "source": [
    "# Unique participant ID column – change if needed\n",
    "ID_COL = \"id\"  # e.g., \"study_id\" if that's what your data uses\n",
    "\n",
    "# AAN presence columns for waves 1–6 (0/1)\n",
    "# >>> EDIT THESE NAMES TO MATCH YOUR DATA <<<\n",
    "AAN_PRESENCE_COLS = {\n",
    "    1: \"w1ONSET-FULL\",\n",
    "    2: \"w2ONSET-FULL-mBMI\",\n",
    "    3: \"w3ONSET-FULL-mBMI\",\n",
    "    4: \"w4ONSET-FULL-mBMI\",\n",
    "    5: \"w5ONSET-FULL-mBMI\",\n",
    "    6: \"w6ONSET-FULL-mBMI\",\n",
    "}\n",
    "\n",
    "# w{x}ed14 and w{x}ed16 column name templates\n",
    "# Assumes columns like \"w4ed14\", \"w4ed16\", \"w5ed14\", etc.\n",
    "ED14_TEMPLATE = \"w{wave}ed14\"\n",
    "ED16_TEMPLATE = \"w{wave}ed16\"\n",
    "\n",
    "print(\"Using AAN presence columns:\", AAN_PRESENCE_COLS)\n",
    "print(\"ID column:\", ID_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9884aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onset (any wave 1–6) counts:\n",
      "aan_onset_any_1_6\n",
      "0    1859\n",
      "1      93\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "labels = raw_df[[ID_COL]].copy()\n",
    "\n",
    "# Ensure presence columns exist and are 0/1\n",
    "for wave, col in AAN_PRESENCE_COLS.items():\n",
    "    labels[col] = raw_df[col].fillna(0).astype(int)\n",
    "\n",
    "# AAN onset at ANY wave (1–6)\n",
    "presence_cols_all = [AAN_PRESENCE_COLS[w] for w in sorted(AAN_PRESENCE_COLS)]\n",
    "labels[\"aan_onset_any_1_6\"] = (labels[presence_cols_all].sum(axis=1) > 0).astype(int)\n",
    "\n",
    "print(\"Onset (any wave 1–6) counts:\")\n",
    "print(labels[\"aan_onset_any_1_6\"].value_counts().rename(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9a9b0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First onset wave distribution:\n",
      "first_onset_wave\n",
      "NaN    1859\n",
      "1.0      34\n",
      "4.0      31\n",
      "3.0      17\n",
      "5.0       8\n",
      "2.0       2\n",
      "6.0       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Persistence anchor wave distribution:\n",
      "persistence_anchor_wave\n",
      "NaN    1859\n",
      "4.0      84\n",
      "5.0       8\n",
      "6.0       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def first_onset_wave(row):\n",
    "    for w in sorted(AAN_PRESENCE_COLS.keys()):\n",
    "        if row[AAN_PRESENCE_COLS[w]] == 1:\n",
    "            return w\n",
    "    return np.nan\n",
    "\n",
    "labels[\"first_onset_wave\"] = labels.apply(first_onset_wave, axis=1)\n",
    "\n",
    "def persistence_anchor_wave(first_w):\n",
    "    \"\"\"\n",
    "    Map first onset wave to the wave at which we check w{x}ed14 / w{x}ed16.\n",
    "    Intervals: 1–4, 4–5, 5–6.\n",
    "    - Onset at 1–4  -> anchor = 4\n",
    "    - Onset at 5    -> anchor = 5\n",
    "    - Onset at 6    -> anchor = 6\n",
    "    \"\"\"\n",
    "    if pd.isna(first_w):\n",
    "        return np.nan\n",
    "    first_w = int(first_w)\n",
    "    if first_w <= 4:\n",
    "        return 4\n",
    "    elif first_w == 5:\n",
    "        return 5\n",
    "    elif first_w == 6:\n",
    "        return 6\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "labels[\"persistence_anchor_wave\"] = labels[\"first_onset_wave\"].apply(persistence_anchor_wave)\n",
    "\n",
    "print(\"First onset wave distribution:\")\n",
    "print(labels[\"first_onset_wave\"].value_counts(dropna=False).rename(\"count\"))\n",
    "\n",
    "print(\"\\nPersistence anchor wave distribution:\")\n",
    "print(labels[\"persistence_anchor_wave\"].value_counts(dropna=False).rename(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "995574b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New persistence label counts (aan_persistence_new):\n",
      "aan_persistence_new\n",
      "0    1925\n",
      "1      27\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Bring in w4ed14, w4ed16, w5ed14, w5ed16, w6ed14, w6ed16 (if present)\n",
    "for wave in (4, 5, 6):\n",
    "    for tmpl in (ED14_TEMPLATE, ED16_TEMPLATE):\n",
    "        col = tmpl.format(wave=wave)\n",
    "        if col in raw_df.columns:\n",
    "            labels[col] = raw_df[col]\n",
    "        else:\n",
    "            print(f\"WARNING: column {col} not found in raw_df; it will be treated as missing.\")\n",
    "\n",
    "def is_persistent(row, missing_as_nonpersistent=True):\n",
    "    \"\"\"\n",
    "    Persistence definition:\n",
    "    - Find persistence_anchor_wave in {4,5,6} (based on first onset).\n",
    "    - Look at w{x}ed14 and w{x}ed16 for that x.\n",
    "    - If BOTH are >= 5, classify as persistent (1).\n",
    "    \"\"\"\n",
    "    w = row[\"persistence_anchor_wave\"]\n",
    "    if pd.isna(w):\n",
    "        return 0\n",
    "    w = int(w)\n",
    "    if w not in (4, 5, 6):\n",
    "        return 0\n",
    "    \n",
    "    ed14_col = ED14_TEMPLATE.format(wave=w)\n",
    "    ed16_col = ED16_TEMPLATE.format(wave=w)\n",
    "    \n",
    "    ed14 = row.get(ed14_col, np.nan)\n",
    "    ed16 = row.get(ed16_col, np.nan)\n",
    "    \n",
    "    if pd.isna(ed14) or pd.isna(ed16):\n",
    "        return 0 if missing_as_nonpersistent else np.nan\n",
    "    \n",
    "    return int((ed14 >= 5) and (ed16 >= 5))\n",
    "\n",
    "labels[\"aan_persistence_new\"] = labels.apply(is_persistent, axis=1).astype(int)\n",
    "\n",
    "print(\"New persistence label counts (aan_persistence_new):\")\n",
    "print(labels[\"aan_persistence_new\"].value_counts().rename(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "771795a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New remission label counts (aan_remission_new):\n",
      "aan_remission_new\n",
      "0    1886\n",
      "1      66\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Course label counts (aan_course_new: 0=none,1=remission,2=persistence):\n",
      "aan_course_new\n",
      "0    1859\n",
      "1      66\n",
      "2      27\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Base remission: onset at any wave (1–6), BUT not persistent\n",
    "labels[\"remission_base\"] = (\n",
    "    (labels[\"aan_onset_any_1_6\"] == 1) &\n",
    "    (labels[\"aan_persistence_new\"] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Extra remission rule:\n",
    "#   AAN in wave 2 or 3, but NOT in wave 4\n",
    "w2_col = AAN_PRESENCE_COLS[2]\n",
    "w3_col = AAN_PRESENCE_COLS[3]\n",
    "w4_col = AAN_PRESENCE_COLS[4]\n",
    "\n",
    "labels[\"remission_early_2_3_no_4\"] = (\n",
    "    ((labels[w2_col] == 1) | (labels[w3_col] == 1)) &\n",
    "    (labels[w4_col] == 0)\n",
    ").astype(int)\n",
    "\n",
    "# Final remission: must NOT be persistent, and satisfy base or early-remission rule\n",
    "labels[\"aan_remission_new\"] = (\n",
    "    (labels[\"aan_persistence_new\"] == 0) &\n",
    "    (\n",
    "        (labels[\"remission_base\"] == 1) |\n",
    "        (labels[\"remission_early_2_3_no_4\"] == 1)\n",
    "    )\n",
    ").astype(int)\n",
    "\n",
    "print(\"New remission label counts (aan_remission_new):\")\n",
    "print(labels[\"aan_remission_new\"].value_counts().rename(\"count\"))\n",
    "\n",
    "# Optional: combined course label\n",
    "# 0 = no onset; 1 = remission; 2 = persistence\n",
    "labels[\"aan_course_new\"] = 0\n",
    "labels.loc[labels[\"aan_remission_new\"] == 1, \"aan_course_new\"] = 1\n",
    "labels.loc[labels[\"aan_persistence_new\"] == 1, \"aan_course_new\"] = 2\n",
    "\n",
    "print(\"\\nCourse label counts (aan_course_new: 0=none,1=remission,2=persistence):\")\n",
    "print(labels[\"aan_course_new\"].value_counts().rename(\"count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ef0bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling cohort (onset only) size: 93\n",
      "Persistence outcome counts:\n",
      "0    66\n",
      "1    27\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Using 24 predictors (after excluding target-related columns).\n"
     ]
    }
   ],
   "source": [
    "# Merge labels onto feature_df by ID\n",
    "model_df = feature_df.merge(\n",
    "    labels[[ID_COL, \"aan_onset_any_1_6\", \"aan_persistence_new\"]],\n",
    "    on=ID_COL,\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# Restrict to participants with AAN onset (1–6)\n",
    "model_df = model_df[model_df[\"aan_onset_any_1_6\"] == 1].copy()\n",
    "\n",
    "y = model_df[\"aan_persistence_new\"].astype(int).values\n",
    "\n",
    "print(\"Modeling cohort (onset only) size:\", len(model_df))\n",
    "print(\"Persistence outcome counts:\")\n",
    "print(pd.Series(y).value_counts().rename(\"count\"))\n",
    "\n",
    "# Drop predictors directly tied to the label to avoid leakage\n",
    "target_related_cols = set(AAN_PRESENCE_COLS.values())\n",
    "for wave in (4, 5, 6):\n",
    "    target_related_cols.add(ED14_TEMPLATE.format(wave=wave))\n",
    "    target_related_cols.add(ED16_TEMPLATE.format(wave=wave))\n",
    "\n",
    "all_features = feature_sets[\"all_features\"]\n",
    "predictor_cols = [c for c in all_features if c not in target_related_cols]\n",
    "\n",
    "print(f\"\\nUsing {len(predictor_cols)} predictors (after excluding target-related columns).\")\n",
    "\n",
    "# Simple median imputation\n",
    "X = model_df[predictor_cols].copy()\n",
    "X = X.fillna(X.median(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1c577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
