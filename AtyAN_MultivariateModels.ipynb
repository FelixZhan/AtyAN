{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure dependencies are installed when running in hosted notebooks",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a0cfc",
   "metadata": {},
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://raw.githubusercontent.com/FelixZhan/AtyAN/main/\"\n",
    "HELPER_FILES = [\n",
    "    \"analysis_utils.py\",\n",
    "    \"requirements.txt\",\n",
    "    \"BP1234-ONSET.csv\",\n",
    "]\n",
    "\n",
    "for filename in HELPER_FILES:\n",
    "    dest = Path(filename)\n",
    "    if dest.exists():\n",
    "        print(f'{filename} already present, skipping download.')\n",
    "        continue\n",
    "    print(f'Downloading {filename}...')\n",
    "    urllib.request.urlretrieve(f\"{BASE_URL}{filename}\", dest)\n",
    "print('Helper files are ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f844ab",
   "metadata": {},
   "source": [
    "## Dataset and design matrix\n",
    "\n",
    "By default the multivariate models target future atypical AN onset (waves 1\u20136). Set `RUN_PERSISTENCE_MODELS = True` to reuse the persistence/remission cohort instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcf5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import (\n",
    "    load_base_dataset,\n",
    "    engineer_baseline_features,\n",
    "    prepare_univariate_prediction_dataset,\n",
    "    prepare_persistence_dataset,\n",
    "    evaluate_model_zoo,\n",
    ")\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc8f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PERSISTENCE_MODELS = False  # Toggle to True to target persistence instead of onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22442cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = load_base_dataset()\n",
    "feature_df, feature_sets = engineer_baseline_features(raw_df)\n",
    "\n",
    "if RUN_PERSISTENCE_MODELS:\n",
    "    design_df = prepare_persistence_dataset(feature_df, feature_sets['all_features'])\n",
    "    target_label = 'aan_persistence'\n",
    "    cohort_label = 'persistence/remission'\n",
    "else:\n",
    "    design_df = prepare_univariate_prediction_dataset(feature_df, feature_sets['all_features'])\n",
    "    target_label = 'aan_onset_anywave'\n",
    "    cohort_label = 'onset prediction'\n",
    "\n",
    "print(f\"Design matrix shape: {design_df.shape}\")\n",
    "print(f\"Target: {target_label} ({cohort_label})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c10fa74",
   "metadata": {},
   "source": [
    "## Model comparison summary\n",
    "\n",
    "Each pipeline uses stratified repeated holdout splits and now reports AUROC, accuracy, sensitivity, specificity, F-score, and G-mean alongside the previous diagnostics. All available models run by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fa742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics, split_tables, feature_tables, model_errors = evaluate_model_zoo(\n",
    "    design_df,\n",
    "    feature_sets['all_features'],\n",
    "    target_col=target_label,\n",
    "    model_names=None,\n",
    ")\n",
    "if model_metrics.empty:\n",
    "    print('No models could be evaluated.')\n",
    "else:\n",
    "    metric_order = [\n",
    "        'test_roc_auc_mean',\n",
    "        'test_accuracy_mean',\n",
    "        'test_sensitivity_mean',\n",
    "        'test_specificity_mean',\n",
    "        'test_f_score_mean',\n",
    "        'test_g_mean_mean',\n",
    "    ]\n",
    "    present = ['model'] + [c for c in metric_order if c in model_metrics.columns]\n",
    "    display(model_metrics[present])\n",
    "if not model_metrics.empty and 'overfit_flag' in model_metrics.columns:\n",
    "    flagged = model_metrics[model_metrics['overfit_flag']]\n",
    "    if not flagged.empty:\n",
    "        for name in flagged['model']:\n",
    "            display(split_tables[name])\n",
    "for name, err in model_errors.items():\n",
    "    print(f'{name}: {err}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c3cd7",
   "metadata": {},
   "source": [
    "## Feature importance snapshots\n",
    "\n",
    "Tree-based models expose feature importances (or absolute coefficients for logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, table in feature_tables.items():\n",
    "    print(f'Top features for {model_name}')\n",
    "    display(table.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}